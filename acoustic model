#acoustic model(1)
import librosa
import numpy as np

def extract_mfccs(audio_paths, num_mfcc=13, n_fft=2048, hop_length=512, max_frames=None):
    all_mfccs = []
    for audio_path in audio_paths:
        # Load audio file
        audio, sr = librosa.load(audio_path, sr=None)

        # Extract MFCCs
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)

        # Truncate or pad MFCCs to ensure consistent number of frames
        if max_frames is not None:
            if mfccs.shape[1] < max_frames:
                mfccs = np.pad(mfccs, ((0, 0), (0, max_frames - mfccs.shape[1])), mode='constant')
            elif mfccs.shape[1] > max_frames:
                mfccs = mfccs[:, :max_frames]

        # Append to the list of MFCCs
        all_mfccs.append(mfccs)

    return all_mfccs

# Example usage with multiple audio files
audio_paths = ["/content/drive/MyDrive/Data/rapoduoutput.wav", "/content/drive/MyDrive/Data/mulioutput.wav",
               "/content/drive/MyDrive/Data/nayioutput.wav","/content/drive/MyDrive/Data/toluoutput.wav"]

# Set the maximum number of frames (adjust as needed)
max_frames = 200

mfccs_list = extract_mfccs(audio_paths, max_frames=max_frames)

# Print shape of extracted MFCCs for each audio file
for i, mfccs in enumerate(mfccs_list):
    print("MFCCs shape for audio file {}: {}".format(i+1, mfccs.shape))
    np.savetxt('mfccs{}.csv'.format(i+1), mfccs, delimiter=',')

#acoustic model train(2)
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load MFCC features from CSV files
mfccs1 = np.loadtxt('mfccs1.csv', delimiter=',')
mfccs2 = np.loadtxt('mfccs2.csv', delimiter=',')
mfccs3 = np.loadtxt('mfccs3.csv', delimiter=',')
mfccs4 = np.loadtxt('mfccs4.csv', delimiter=',')

# Concatenate MFCC features from all audio files
X = np.vstack([mfccs1, mfccs2, mfccs3, mfccs4])

# Prepare target labels (phonetic transcriptions)
y = np.array(['Night'] * mfccs1.shape[0] + ['Knee'] * mfccs2.shape[0]+ ['Dog'] * mfccs3.shape[0] + ['Skin'] * mfccs4.shape[0])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Load the new audio and extract MFCC features
new_audio_path = "/content/drive/MyDrive/Data/rapodhumulinayiop.wav"
new_mfccs = extract_mfccs([new_audio_path], max_frames=200)

# Verify the shape of the extracted MFCC features
print("Shape of new MFCCs:", new_mfccs[0].shape)

# Predict the phonetic transcription using the trained model
predicted_transcription = clf.predict(new_mfccs[0])

print("Predicted transcription:", predicted_transcription)

#output:Shape of new MFCCs: (13, 200)
#Predicted transcription: ['Dog' 'Dog' 'Dog' 'Dog' 'Dog' 'Dog' 'Night' 'Night' 'Knee' 'Dog' 'Dog' 'Dog' 'Dog']
